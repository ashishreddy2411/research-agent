# Deep Research Agent â€” Built in Raw Python

> A complete, working Deep Research Agent built without any agent framework â€” giving you full visibility and control over every layer of the system.

---

## What This Project Is

A system that takes a plain English question, plans a research strategy, searches the web, reads pages, compresses context, reflects on gaps, searches again, and synthesizes a cited structured report.

Built entirely in raw Python. No LangChain. No LangGraph. No GPT-Researcher. Every component â€” the research loop, the reflect-search iteration, the context compression, the synthesis pipeline â€” is written from scratch. No framework hiding what's actually happening.

---

## What This Covers

| Concept | How it works here |
|---|---|
| Planning | Planner decomposes the question into targeted subqueries |
| Tools | search, fetch, extract, summarize â€” each a clean isolated module |
| Context management | 50-100 pages compressed via cheap model + relevance filter |
| Stopping condition | Reflector decides when coverage is sufficient |
| Output | Multi-section Markdown report with inline `[N]` citations + References section |
| Cost control | Hard cap â€” cost checked before every round |
| Observability | Span-based tracing for every pipeline step; saved to `logs/traces/` |
| Guardrails | Input validation, SSRF-safe URL checks, citation bounds, query dedup |
| Evaluations | Keyword recall, citation accuracy, citation density, composite score |
| Web UI | Streamlit app â€” Ask tab, Dashboard tab, Traces tab |

---

## Architecture

```
User Question
        â”‚
        â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Guardrails: validate_query()        â”‚
â”‚  Reject empty / too short / too long â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
        â”‚
        â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Planner                             â”‚
â”‚  LLM decomposes into 3-5 subqueries  â”‚
â”‚  deduplicate_queries() removes dupes â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
        â”‚
        â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Research Loop  (up to max_research_rounds)               â”‚
â”‚                                                           â”‚
â”‚  For each subquery:                                       â”‚
â”‚    Researcher:                                            â”‚
â”‚      tavily.search(query) â†’ URLs + page content          â”‚
â”‚      is_safe_url() check before every fetch              â”‚
â”‚      cheap_llm.summarize(page) â†’ 200-word bullets        â”‚
â”‚      â†’ PageSummary added to ResearchState                â”‚
â”‚                                                           â”‚
â”‚  Reflector: "What gaps remain? Follow-up query?"         â”‚
â”‚    â†’ gap found â†’ next round with follow-up query         â”‚
â”‚    â†’ no gap OR max rounds â†’ stop                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
        â”‚
        â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Synthesizer (two-shot)              â”‚
â”‚  Shot 1: outline (section headings)  â”‚
â”‚  Shot 2: full report, [N] citations  â”‚
â”‚  check_citation_bounds() validates   â”‚
â”‚  References section auto-appended    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
        â”‚
        â–¼
    ResearchState returned
    (final_report, sources, cost_usd, rounds, status, spans)
```

---

## Project Structure

```
research-agent/
â”‚
â”œâ”€â”€ agent/
â”‚   â”œâ”€â”€ state.py          # ResearchState dataclass + ResearchStatus enum
â”‚   â”œâ”€â”€ planner.py        # query decomposition â€” one question â†’ N subqueries
â”‚   â”œâ”€â”€ researcher.py     # search + fetch + summarize for one subquery
â”‚   â”œâ”€â”€ reflector.py      # gap detection â€” should we search again?
â”‚   â”œâ”€â”€ synthesizer.py    # two-shot outline â†’ report with mandatory citations
â”‚   â”œâ”€â”€ guardrails.py     # validate_query, is_safe_url, check_citation_bounds, dedup
â”‚   â””â”€â”€ loop.py           # orchestrates the full pipeline; never raises
â”‚
â”œâ”€â”€ prompts/
â”‚   â”œâ”€â”€ planner.py        # DECOMPOSE_PROMPT
â”‚   â”œâ”€â”€ reflector.py      # REFLECT_PROMPT
â”‚   â””â”€â”€ synthesizer.py    # OUTLINE_PROMPT, REPORT_PROMPT
â”‚
â”œâ”€â”€ tools/
â”‚   â”œâ”€â”€ search.py         # Tavily API wrapper â†’ list[SearchResult]
â”‚   â”œâ”€â”€ fetch.py          # Jina Reader + trafilatura â†’ FetchResult (SSRF-safe)
â”‚   â””â”€â”€ extract.py        # HTML â†’ clean text, truncation helpers
â”‚
â”œâ”€â”€ llm/
â”‚   â””â”€â”€ client.py         # Azure AI Foundry wrapper: generate, generate_cheap, embed
â”‚                         # tracks token usage + cost per call
â”‚
â”œâ”€â”€ observability/
â”‚   â”œâ”€â”€ tracer.py         # Span + Trace dataclasses; context-manager instrumentation
â”‚   â””â”€â”€ dashboard.py      # load_traces, summary_stats, latency_stats, cost_stats
â”‚
â”œâ”€â”€ evals/
â”‚   â”œâ”€â”€ dataset.py        # 5 eval questions with ground-truth keywords
â”‚   â”œâ”€â”€ metrics.py        # citation_accuracy, citation_density, keyword_coverage,
â”‚   â”‚                     # source_quality, run_score (composite)
â”‚   â””â”€â”€ runner.py         # CLI runner â€” coloured output, summary table, JSON export
â”‚
â”œâ”€â”€ tests/
â”‚   â”œâ”€â”€ unit/             # 314 tests, no API calls required
â”‚   â”‚   â”œâ”€â”€ test_state.py
â”‚   â”‚   â”œâ”€â”€ test_planner.py
â”‚   â”‚   â”œâ”€â”€ test_reflector.py
â”‚   â”‚   â”œâ”€â”€ test_researcher.py
â”‚   â”‚   â”œâ”€â”€ test_loop.py
â”‚   â”‚   â”œâ”€â”€ test_guardrails.py
â”‚   â”‚   â”œâ”€â”€ test_synthesizer.py
â”‚   â”‚   â”œâ”€â”€ test_tracer.py
â”‚   â”‚   â”œâ”€â”€ test_dashboard.py
â”‚   â”‚   â”œâ”€â”€ test_fetch.py
â”‚   â”‚   â”œâ”€â”€ test_search.py
â”‚   â”‚   â””â”€â”€ test_extract.py
â”‚   â””â”€â”€ integration/      # end-to-end tests (needs real API keys)
â”‚
â”œâ”€â”€ app.py                # Streamlit UI â€” Ask, Dashboard, Traces tabs
â”œâ”€â”€ config.py             # pydantic-settings â€” all env vars typed + validated
â”œâ”€â”€ pyproject.toml
â””â”€â”€ .env.example
```

---

## Setup

### Prerequisites
- Python 3.12+
- [uv](https://docs.astral.sh/uv/) installed
- Microsoft Azure AI Foundry project with models deployed
- Tavily API key (free at [app.tavily.com](https://app.tavily.com))

### Models needed in Azure Foundry
| Role | Used for |
|---|---|
| Smart (e.g. `gpt-4o`) | Planning, reflection, synthesis |
| Cheap (e.g. `gpt-4o-mini`) | Per-page summarization (50-100x per run) |
| Embeddings (e.g. `text-embedding-3-small`) | Context relevance filtering |

### Install

```bash
cd research-agent
uv sync
```

### Configure

```bash
cp .env.example .env
# Edit .env with your Azure AI Foundry and Tavily credentials
```

### Run unit tests (no API keys needed)

```bash
uv run pytest tests/unit/ -v
# 314 passed
```

### Run the Streamlit UI

```bash
uv run streamlit run app.py
# Opens at http://localhost:8501
```

### Run from Python

```python
from agent.loop import run_research

state = run_research(
    "What are the major breakthroughs in solid-state battery technology in 2024?",
    on_progress=print,   # stream progress to stdout
)
print(state.final_report)
print(f"Sources: {len(state.sources)}  Cost: ${state.estimated_cost_usd:.4f}")
```

### Run evaluations (needs API keys)

```bash
# Run all 5 eval questions
uv run python -m evals.runner

# Run one question by category
uv run python -m evals.runner --category science

# Save results to JSON
uv run python -m evals.runner --output results.json
```

---

## Build Status

| Phase | Status | What it builds |
|---|---|---|
| 1 â€” Foundation | âœ… Complete | Search, fetch, extract tools + integration tests |
| 2 â€” Research Loop | âœ… Complete | Planner, researcher, reflector, ResearchState, prompts folder |
| 3 â€” Synthesizer | âœ… Complete | Two-shot outline â†’ report, mandatory `[N]` citations, References section |
| 4 â€” Observability | âœ… Complete | Span-based tracer, dashboard metrics, cost tracking in LLMClient |
| 5 â€” Streamlit UI | âœ… Complete | Ask + Dashboard + Traces tabs, live progress via `on_progress` callback |
| Pre-6 Audit | âœ… Complete | Guardrails at every layer, 314 unit tests, eval framework |
| 6 â€” Production Ready | ğŸ”² Planned | Parallel fetching, checkpoint + resume, streaming, job queue |

---

## Tech Stack

| Layer | Tool |
|---|---|
| LLM | GPT-4o + GPT-4o-mini via Microsoft Azure AI Foundry |
| Search | Tavily API |
| URL fetching | Jina Reader + trafilatura |
| Agent framework | Raw Python â€” no LangChain, no LangGraph |
| Config | `pydantic-settings` v2 |
| Web UI | Streamlit |
| Package manager | `uv` |
| Tests | `pytest` (314 unit tests) |

---

*Last updated: February 2026*
